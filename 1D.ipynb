{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was extremely rude and really, there are so many other restaurants I would love to dine at during a weekend in Vegas. 23\n",
      "Steer clear of this product and go with the genuine Palm replacementr pens, which come in a three-pack. 18\n",
      "There is really nothing for me at postinos, hope your experience is better 13\n",
      "The scallop dish is quite appalling for value as well. 10\n",
      "I was left shattered from the experience of watching this 'film' and I took a nice two hours to fully recover.   21\n",
      "Anyways, The food was definitely not filling at all, and for the price you pay you should expect more. 19\n",
      "I'm super pissd. 3\n",
      "Although I very much liked the look and sound of this place, the actual experience was a bit disappointing. 19\n",
      "I could not understand, what kind of idiot would produce this mess in the first place not to mention several season.   21\n",
      "Att is not clear, sound is very distorted and you have to yell when you talk. 16\n",
      "They refuse to refund or replace. 6\n",
      "Graphics is far from the best part of the game.   10\n",
      "The plastic breaks really easy on this clip. 8\n",
      "I have two more years left in this contract and I hate this phone. 14\n",
      "It was a pale color instead of nice and char and has NO flavor. 14\n",
      "It was a very superficial movie and it gave me the feeling that I was watching play rather than a film.   21\n",
      "The movie was so boring, that I sometimes found myself occupied peaking in the paper instead of watching (never happened during a Columbo movie before!   25\n",
      "And, quite honestly, often its not very nice.   8\n",
      "The only place nice for this film is in the garbage.   11\n",
      "20th Century Fox's ROAD HOUSE 1948) is not only quite a silly noir but is an implausible unmitigated bore of a movie.   22\n",
      "I really wanted the Plantronics 510 to be the right one, but it has too many issues for me.The nice 20\n",
      "------------\n",
      "It does everything the description said it would. 8\n",
      "Better Than New. 3\n",
      "After one bite, I was hooked. 6\n",
      "The wontons were thin, not thick and chewy, almost melt in your mouth. 13\n",
      "i'm glad i found this product on amazon it is hard to find, it wasn't high priced. 17\n",
      "The ambiance was incredible. 4\n",
      "Arrived quickly and much less expensive than others being sold. 10\n",
      "The portion was huge! 4\n",
      "If there was ever a movie that needed word-of-mouth to promote, this is it.   14\n",
      "They keep getting better and better (this is my third one and I've had numerous Palms too). 17\n",
      "A world better than 95% of the garbage in the theatres today.   12\n",
      "One of the few places in Phoenix that I would definately go back to again . 16\n",
      "Highly recommended for all ages, although the younger set will probably not appreciate some of the more subtle references, they will certainly appreciate one galley scene in particular!   28\n",
      "Gets a signal when other Verizon phones won't. 8\n",
      "He deserves 5 stars. 4\n",
      "I was seated immediately. 4\n",
      "Highly recommended. 2\n",
      "Never had anything to complain about here. 7\n",
      "Cute, quaint, simple, honest. 4\n",
      "Leopard Print is wonderfully wild!. 5\n",
      "I recently tried Caballero's and I have been back every week since! 12\n",
      "He's a national treasure.   4\n",
      "This was my first time and I can't wait until the next. 12\n",
      "They could be used as exemplars for any set designer.   10\n",
      "Would come back again if I had a sushi craving while in Vegas. 13\n",
      "Having to humour him just to get by and get through the day was so apt.   16\n",
      "The goat taco didn't skimp on the meat and wow what FLAVOR! 12\n",
      "While you don't yet hear Mickey speak, there are tons of sound effects and music throughout the film--something we take for granted now but which was a huge crowd pleaser in 1928.   32\n",
      "This movie creates its own universe, and is fascinating in every way.   12\n",
      "I'm glad the film didn't go for the most obvious choice, as a lesser film certainly would have.   18\n",
      "Raw and sublimely moving.   4\n",
      "The battery is working well as a replacement for the original that came with the phone over 2 years ago. 20\n",
      "This is a witty and delightful adaptation of the Dr Seuss book, brilliantly animated by UPA's finest and thoroughly deserving of its Academy Award.   24\n",
      "Garbo, who showed right off the bat that her talents could carry over from the silent era (I wanted to see some of her silent work, but Netflix doesn't seem to be stocking them.   34\n",
      "I really like this product over the Motorola because it is allot clearer on the ear piece and the mic. 20\n",
      "I can't wait to go back. 6\n",
      "In addition to having one of the most lovely songs ever written, French Cancan also boasts one of the cutest leading ladies ever to grace the screen.   27\n",
      "It is so small and you don't even realize that it is there after a while of getting used to it. 21\n",
      "The nachos are a MUST HAVE! 6\n",
      "Five star Plus, plus. 4\n",
      "It was a riot to see Hugo Weaving play a sex-obsessed gay real estate salesman who uses his clients' houses for his trysts with the flaming Darren (Tom Hollander).   29\n",
      "I believe every one should see this movie as I think few people outside of South Africa understand its past and what is being attempted in the Truth and Reconciliation process.   31\n",
      "I can assure you that you won't be disappointed. 9\n",
      "Eclectic selection. 2\n",
      "every thing on phone work perfectly, she like it. 9\n",
      "Cooked to perfection and the service was impeccable. 8\n",
      "I have recommended it to friends.   6\n",
      "Helen Baxendale is a very credible lady Macbeth who can be very cheerfull at times and sometimes she just looks like a naughty girl, but deadly in her taste for blood and evil.   33\n",
      "Their steaks are 100% recommended! 5\n",
      ":-)Oh, the charger seems to work fine. 7\n",
      "After trying many many handsfree gadgets this is the one that finally works well. 14\n",
      "The pairing of the two devices was so easy it barely took a couple minutes before I started making calls with the voice dialing feature. 25\n",
      "Only had this a month but it's worked flawlessly so far. 11\n",
      "Check it out. 3\n",
      "The cast of veteran actors are more than just a nostalgia trip.   12\n",
      "Very wind-resistant. 2\n",
      "The lighting is just dark enough to set the mood. 10\n",
      "You can't beat the price on these. 7\n",
      "And there wasn't a single sour note struck acting-wise, either; some surprisingly solid casting, here.   15\n",
      "The warmth it generates is in contrast to its austere backdrop.   11\n",
      "I have a Verizon LG phone and they work well together, nice reception and range that exceeds 20 feet line of sight. 22\n",
      "Once you get inside you'll be impressed with the place. 10\n",
      "Battery charge-life is quite long. 5\n",
      "Appears to actually outperform the original battery from China that came with my V325i. 14\n",
      "I will never forget it now.   6\n",
      "Julian Fellowes has triumphed again.   5\n",
      "Service is also cute. 4\n",
      "Virgin Wireless rocks and so does this cheap little phone! 10\n",
      "This is an unbelievable BARGAIN! 5\n",
      "15.333333333333334 11.927536231884059 11.83448275862069 11.745454545454546\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from cross_validation import *\n",
    "\n",
    "BOW_FILE = \"bow_columns_list.txt\"\n",
    "REGRESSION_PKL_FILE = \"regression_mdl.pkl\"\n",
    "WEBSITE_MAPPING = {'imdb': 0, 'amazon': 1, 'yelp': 2}\n",
    "RANDOM_STATE = 132\n",
    "NUM_FOLDS = 6\n",
    "\n",
    "### IO\n",
    "import pickle\n",
    "REGRESSION_PKL_FILE = \"regression_mdl.pkl\"\n",
    "\n",
    "def write_out_model(model, filename=REGRESSION_PKL_FILE):\n",
    "    with open(REGRESSION_PKL_FILE, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        print(f'model saved as: {REGRESSION_PKL_FILE}')\n",
    "\n",
    "def read_in_model(filename=REGRESSION_PKL_FILE):\n",
    "    with open(REGRESSION_PKL_FILE, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "def copy_model_hyperparameters(model):\n",
    "    return sklearn.linear_model.LogisticRegression(solver=model.solver, C=model.C, fit_intercept=model.fit_intercept, penalty=model.penalty, tol=model.tol)\n",
    "\n",
    "### BOW\n",
    "def make_bag_of_words_from_vocab(text_series, ngram_range=(1, 1), max_features=1000, vocabulary_file=BOW_FILE, binary=True):\n",
    "    with open(vocabulary_file, 'r') as file:\n",
    "        vocabulary = file.read().splitlines()\n",
    "    text_series = [text_list[1] for text_list in text_series]\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features, stop_words='english', vocabulary=vocabulary, binary=binary)\n",
    "    bow_matrix = vectorizer.fit_transform(text_series)\n",
    "    bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return bow_df\n",
    "\n",
    "def make_bag_of_words(text_series, ngram_range=(1, 1), max_features=1000, binary=True):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features, stop_words='english', binary=binary)\n",
    "    bow_matrix = vectorizer.fit_transform(text_series)\n",
    "    bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return bow_df\n",
    "\n",
    "def make_bags_of_words(text_series, smallest, largest, max_features=None):\n",
    "    df = pd.DataFrame(index=range(smallest, largest+1), columns=range(smallest, largest+1))\n",
    "    for i in range(smallest, largest+1):\n",
    "        for j in range(i, largest+1):\n",
    "            df[i][j] = make_bag_of_words(text_series, ngram_range=(i, j), max_features=max_features)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "### TRAINING\n",
    "def find_best_hyperparameters(x_train_array, y_train_array, serialize=True):\n",
    "    # penalties = ['l1', 'l2', 'elasticnet']\n",
    "    penalties = ['l2']\n",
    "    # C_grid = np.logspace(0, np.log10(2), 10)\n",
    "    C_grid = [1.0284017999892119]\n",
    "    # solvers = ['lbfgs', 'liblinear']\n",
    "    solvers = ['liblinear']\n",
    "    # fit_intercepts = [True, False]\n",
    "    fit_intercepts = [True]\n",
    "    # tols = np.logspace(-2, 0, 10)\n",
    "    tols = [0.4146299840754365]\n",
    "\n",
    "    total_iterations = len(tols) * len(fit_intercepts) * len(solvers) * len(C_grid) * len(penalties)\n",
    "    print(f'total iterations: {total_iterations}')\n",
    "    models_and_hyperparameters = []\n",
    "    curr_itr = 0\n",
    "    for penalty in penalties:\n",
    "        for C in C_grid:\n",
    "            for solver in solvers:\n",
    "                for fit_intercept in fit_intercepts:\n",
    "                    for tol in tols:\n",
    "                        curr_itr += 1\n",
    "                        model = sklearn.linear_model.LogisticRegression(penalty=penalty, C=C, solver=solver, fit_intercept=fit_intercept, tol=tol, max_iter=1000)\n",
    "                        model.fit(x_train_array, y_train_array.ravel())\n",
    "                        avg_test_error, rocauc, avg_train_error = get_error_and_rocauc(model, x_train_array, y_train_array)\n",
    "                        models_and_hyperparameters.append((model, penalty, C, solver, fit_intercept, tol, avg_test_error, rocauc))\n",
    "                        print(f'{curr_itr}/{total_iterations}', penalty, C, solver, fit_intercept, tol, avg_test_error, rocauc)\n",
    "\n",
    "    min_err = 1000000\n",
    "    best_model_tuple = ()\n",
    "    for models_and_hyperparameter in models_and_hyperparameters:\n",
    "        curr_err = models_and_hyperparameter[6]\n",
    "        if curr_err < min_err:\n",
    "            min_err = curr_err\n",
    "            best_model_tuple = models_and_hyperparameter\n",
    "    \n",
    "    if serialize:\n",
    "        best_model = best_model_tuple[0]\n",
    "        print(best_model_tuple)\n",
    "        write_out_model(best_model)\n",
    "    return best_model_tuple\n",
    "\n",
    "\n",
    "\n",
    "### TESTING\n",
    "def get_error_and_rocauc(model, x_train_array, y_train_array, num_folds=NUM_FOLDS, random_state=RANDOM_STATE):\n",
    "    train_err_per_fold, test_error_per_fold = train_models_and_calc_scores_for_n_fold_cv(model, x_train_array, y_train_array.ravel(), n_folds=num_folds, random_state=random_state)\n",
    "    avg_test_error = np.average(test_error_per_fold)\n",
    "    avg_train_error = np.average(train_err_per_fold)\n",
    "    y_train_proba = model.predict_proba(x_train_array)[:,1]\n",
    "    rocauc = sklearn.metrics.roc_auc_score(y_train_array, y_train_proba)\n",
    "    return avg_test_error, rocauc, avg_train_error\n",
    "\n",
    "\n",
    "def copy_and_test_model(model, x_train_array, y_train_array, num_folds=NUM_FOLDS, random_state=RANDOM_STATE):\n",
    "    model = copy_model_hyperparameters(model)\n",
    "    print(model, get_error_and_rocauc(model, x_train_array, y_train_array))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ MAIN #############\n",
    "def main(data_dir='./data_reviews'):\n",
    "    x_train_df = pd.read_csv(os.path.join(data_dir, 'x_train.csv'))\n",
    "    y_train_df = pd.read_csv(os.path.join(data_dir, 'y_train.csv'))\n",
    "    text = x_train_df['text']\n",
    "    \n",
    "    # choose bow\n",
    "    bow_max_features = 1000\n",
    "    bow = make_bag_of_words(x_train_df['text'], max_features=bow_max_features)\n",
    "    # export as list\n",
    "    with open(BOW_FILE, 'w') as file:\n",
    "        for column in bow.columns:\n",
    "            file.write(column + '\\n')\n",
    "\n",
    "    # make final training data\n",
    "    x_train_df = pd.concat([x_train_df, bow], axis=1).drop(columns=['text', 'website_name'], axis=1)\n",
    "    # x_train_df['website_name'].replace(website_mapping, inplace=True)\n",
    "    x_train_array = x_train_df.to_numpy()\n",
    "    y_train_array = y_train_df.to_numpy()\n",
    "    \n",
    "    # # find best hyperparameters\n",
    "    # best_model_and_hyperparameters = find_best_hyperparameters(x_train_array=x_train_array, y_train_array=y_train_array, serialize=True)\n",
    "\n",
    "    # # test with more words\n",
    "    # x_train_df = pd.read_csv(os.path.join(data_dir, 'x_train.csv'))\n",
    "    # bow_max_features = 1000\n",
    "    # bow = make_bag_of_words(x_train_df['text'], max_features=bow_max_features)\n",
    "    # x_train_df = pd.concat([x_train_df, bow], axis=1).drop(columns=['text', 'website_name'], axis=1)\n",
    "    # x_train_array = x_train_df.to_numpy()\n",
    "    # y_train_array = y_train_df.to_numpy()\n",
    "    # copy_and_test_model(best_model_and_hyperparameters[0], x_train_array, y_train_array)\n",
    "    # write_out_model(best_model_and_hyperparameters[0])\n",
    "\n",
    "    # use prior model\n",
    "    # copy_and_test_model(read_in_model(), x_train_array, y_train_array)\n",
    "\n",
    "    # current best model\n",
    "    model = sklearn.linear_model.LogisticRegression(C=1.0284017999892119, solver='liblinear', tol=0.4146299840754365, fit_intercept=True, penalty='l2')\n",
    "    # write_out_model(model)\n",
    "\n",
    "    # 1D\n",
    "    explore_word_count(model, x_train_array, y_train_array, text)\n",
    "\n",
    "\n",
    "def explore_word_count(model, x_train_array, y_train_array, text):\n",
    "    tp_word_count = []\n",
    "    tn_word_count = []\n",
    "    fp_word_count = []\n",
    "    fn_word_count = []\n",
    "    for fold_number in [2]:\n",
    "        train_ids_per_fold, test_ids_per_fold = make_train_and_test_row_ids_for_n_fold_cv(n_examples=x_train_array.shape[0], n_folds=NUM_FOLDS, random_state=RANDOM_STATE)\n",
    "        model.fit(x_train_array[train_ids_per_fold[fold_number]], y_train_array[train_ids_per_fold[fold_number]].ravel())\n",
    "\n",
    "        tp_indices = []\n",
    "        tn_indices = []\n",
    "        fp_indices = []\n",
    "        fn_indices = []\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "        for index in test_ids_per_fold[fold_number]:\n",
    "            prediction = model.predict([x_train_array[index]])\n",
    "            if prediction == y_train_array[index]:\n",
    "                if y_train_array[index] == 1:\n",
    "                    tp_indices.append(index)\n",
    "                else:\n",
    "                    tn_indices.append(index)\n",
    "                hits += 1\n",
    "            else:\n",
    "                if y_train_array[index] == 1:\n",
    "                    fn_indices.append(index)\n",
    "                else:\n",
    "                    fp_indices.append(index)\n",
    "                misses += 1\n",
    "        \n",
    "        fp_size_total = 0\n",
    "        fn_size_total = 0\n",
    "        tp_size_total = 0\n",
    "        tn_size_total = 0\n",
    "        for index in fp_indices:\n",
    "            fp_size_total += len(text[index].split())\n",
    "            print(text[index], len(text[index].split()))\n",
    "        print('------------')\n",
    "        for index in fn_indices:\n",
    "            fn_size_total += len(text[index].split())\n",
    "            print(text[index], len(text[index].split()))\n",
    "        for index in tp_indices:\n",
    "            tp_size_total += len(text[index].split())\n",
    "        for index in tn_indices:\n",
    "            tn_size_total += len(text[index].split())\n",
    "        # miss_word_count_tuples.append((fp_size_total / len(fp_indices), fn_size_total / len(fn_indices)))\n",
    "        # hit_word_count_tuples.append((tp_size_total / len(tp_indices), tn_size_total / len(tn_indices)))\n",
    "        fp_word_count.append(fp_size_total / len(fp_indices))\n",
    "        fn_word_count.append(fn_size_total / len(fn_indices))\n",
    "        tp_word_count.append(tp_size_total / len(tp_indices))\n",
    "        tn_word_count.append(tn_size_total / len(tn_indices))\n",
    "    for i in range(1):\n",
    "        print(fp_word_count[i], fn_word_count[i], tp_word_count[i], tn_word_count[i])\n",
    "    # x_values = range(len(fp_word_count))\n",
    "\n",
    "    # # Plotting the values\n",
    "    # plt.plot(x_values, fp_word_count, label='FP')\n",
    "    # plt.plot(x_values, fn_word_count, label='FN')\n",
    "    # plt.plot(x_values, tp_word_count, label='TP')\n",
    "    # plt.plot(x_values, tn_word_count, label='TN')\n",
    "\n",
    "\n",
    "\n",
    "    # # Show plot\n",
    "    \n",
    "    \n",
    "    # negative_predictions = []\n",
    "    # positive_predictions = []\n",
    "    # for i in range(6):\n",
    "    #     negative_predictions.append((fn_word_count[i] + tn_word_count[i]) / 2)\n",
    "    #     positive_predictions.append((fp_word_count[i] + tp_word_count[i]) / 2)\n",
    "    # plt.plot(x_values, positive_predictions, label=\"Positive Predictions\")\n",
    "    # plt.plot(x_values, negative_predictions, label=\"Negative Predictions\")\n",
    "    # # Adding labels and title\n",
    "    # plt.xlabel('Heldout Fold Used For Testing')\n",
    "    # plt.ylabel('Average Word Count')\n",
    "    # plt.title('Word Count')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
